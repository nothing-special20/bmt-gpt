{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install psycopg2\n",
    "# !pip install psycopg2-binary --user\n",
    "from sqlalchemy import create_engine\n",
    "# engine = create_engine('postgresql://postgres@db:5432/bmt_gpt')\n",
    "from sqlalchemy.engine import URL\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 'NOUN', 'ADJ'\n",
    "def find_types_of_words(text, word_type, nlp=nlp):\n",
    "    doc = nlp(text)\n",
    "    words = [x.text.lower() for x in doc if x.pos_ == word_type]\n",
    "    words = list(set(words))\n",
    "    words.sort()\n",
    "    \n",
    "    return words\n",
    "\n",
    "def most_common_words(text_list, word_type):\n",
    "    all_review_adjectives = []\n",
    "    for review in text_list:\n",
    "        adjs = find_types_of_words(review, word_type)\n",
    "        all_review_adjectives.extend(adjs)\n",
    "\n",
    "    most_common_words = Counter(all_review_adjectives).most_common(15)\n",
    "    return most_common_words\n",
    "\n",
    "sql_type = 'postgresql'\n",
    "host = 'localhost'\n",
    "# host = docker_thing(\"bmt-gpt-saas_default\",\"bmt-gpt-saas_db_1\")\n",
    "# host = \"172.18.0.1'\"\n",
    "port = 5432\n",
    "db = 'bmt_gpt'\n",
    "user = 'postgres'\n",
    "pw = 'postgres'\n",
    "\n",
    "url = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=user,\n",
    "    password=pw,\n",
    "    host=host,\n",
    "    database=db,\n",
    "    port=5432\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(url)\n",
    "connection = engine.connect()\n",
    "# query = text('select * from \"amazon_processedproductreviews\" where \"ASIN_ORIGINAL_ID\" = ' + \"'B07TVK6DC2'\")\n",
    "query = text('select * from \"amazon_processedproductreviews\" where \"ASIN_ORIGINAL_ID\" = ' + \"'B07K32ZCHG'\")\n",
    "processed_product_reviews = pd.read_sql(query, connection)\n",
    "all_reviews = list(processed_product_reviews['REVIEW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I bought these wonderful boxing gloves for my son\")\n",
    "\n",
    "for token in doc:\n",
    "    output = {\n",
    "        'token.text': token.text, \n",
    "        'token.lemma_': token.lemma_, \n",
    "        'token.pos_': token.pos_, \n",
    "        'token.tag_': token.tag_, \n",
    "        'token.dep_': token.dep_,\n",
    "        'token.shape_': token.shape_,\n",
    "        'token.is_alpha': token.is_alpha,\n",
    "        'token.is_stop': token.is_stop\n",
    "    }\n",
    "#     print(json.dumps(output, indent=4))\n",
    "\n",
    "adj = [x.text for x in doc if x.pos_=='ADJ']\n",
    "print(adj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ff389",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_adjs = most_common_words(all_reviews, 'ADJ')\n",
    "most_common_nouns = most_common_words(all_reviews, 'NOUN')\n",
    "most_common_verbs = most_common_words(all_reviews, 'VERB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"I have to go fight someone now\")\n",
    "doc = nlp(\"weekly daily monthly hourly\")\n",
    "\n",
    "for token in doc:\n",
    "    output = {\n",
    "        'token.text': token.text, \n",
    "        'token.lemma_': token.lemma_, \n",
    "        'token.pos_': token.pos_, \n",
    "        'token.tag_': token.tag_, \n",
    "        'token.dep_': token.dep_,\n",
    "        'token.shape_': token.shape_,\n",
    "        'token.is_alpha': token.is_alpha,\n",
    "        'token.is_stop': token.is_stop\n",
    "    }\n",
    "#     print(json.dumps(output, indent=4))\n",
    "#     print(json.dumps(token, indent=4))\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320263e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keybert\n",
    "from keybert import KeyBERT\n",
    "\n",
    "\n",
    "doc = all_reviews[0]\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "kw_model = KeyBERT(model=nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_kws = [kw_model.extract_keywords(rev, keyphrase_ngram_range=(1, 8), diversity=.8, top_n=7) for rev in all_reviews[:20]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print('~', i,' - ', top_kws[i])\n",
    "# for kw in top_kws:\n",
    "    # print('~', kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rev in all_reviews[:100]:\n",
    "    print('~',rev, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('kids')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02320213",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model.extract_keywords('angry wrath sad rage', keyphrase_ngram_range=(1, 3), diversity=.5, top_n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca21322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from bertopic import BERTopic\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "# docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "\n",
    "# docs = '\\n'.join(all_reviews[:100])\n",
    "docs = [str(x) for x in all_reviews]\n",
    "\n",
    "nlp = spacy.load('en_core_web_md', exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
    "\n",
    "topic_model = BERTopic(embedding_model=nlp)\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "fig = topic_model.visualize_topics()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b9323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.visualize_topics()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b132573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(context, question):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"I'm a college student who uses this laptop every day for my assignments and video calls. It's very lightweight and portable, making it perfect for my needs. I usually use it for 6-7 hours a day. The only downside is that the battery life isn't great.\"\n",
    "\n",
    "questions = [\n",
    "    \"Who uses the product?\",\n",
    "    \"How often do they use the product?\",\n",
    "    \"Where do they use the product?\",\n",
    "    \"Why do they like the product?\",\n",
    "    \"Why do they dislike the product?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = extract_information(review, question)\n",
    "    print(f\"{question}: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b352d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97736bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[extract_information(review, question) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in all_reviews[0:5]:\n",
    "    print('~~~', review)\n",
    "    for question in questions:\n",
    "        answer = extract_information(review, question)\n",
    "        print(f\"{question}: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e94a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b288442",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a74e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "test_reviews = all_reviews[0:500]\n",
    "for review in test_reviews:\n",
    "    embedding = generate_embeddings(review)\n",
    "    embeddings.append(embedding.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 10  # Choose the number of clusters/topics\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    centroid = kmeans.cluster_centers_[i]\n",
    "    similarities = cosine_similarity([centroid], embeddings)\n",
    "    top_n_indices = similarities.argsort()[0][-5:]  # Top 5 reviews as examples\n",
    "    print(f\"Cluster {i + 1} representative reviews:\")\n",
    "    for index in top_n_indices:\n",
    "        print('~~~', test_reviews[index])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d165f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_adjs = most_common_words(test_reviews, 'ADJ')\n",
    "most_common_nouns = most_common_words(test_reviews, 'NOUN')\n",
    "most_common_verbs = most_common_words(test_reviews, 'VERB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_thing = [x for x in test_reviews if 'comfortable' in x]\n",
    "for x in test_thing:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ded77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd96453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8)\n",
    "tfidf_matrix = vectorizer.fit_transform(test_reviews)\n",
    "\n",
    "num_clusters = 5  # Choose the number of clusters/topics\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(f\"Cluster {i + 1} top keywords:\")\n",
    "    for index in order_centroids[i, :10]:  # Top 10 keywords\n",
    "        print(terms[index])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbddf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2d8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [', '.join(find_types_of_words(rev, 'NOUN', nlp=nlp)) for rev in test_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e311948",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8)\n",
    "tfidf_matrix = vectorizer.fit_transform(nouns)\n",
    "\n",
    "num_clusters = 10  # Choose the number of clusters/topics\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(f\"Cluster {i + 1} top keywords:\")\n",
    "    for index in order_centroids[i, :10]:  # Top 10 keywords\n",
    "        print(terms[index])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229172f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmt-doc-processing",
   "language": "python",
   "name": "bmt-doc-processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "593115c5125862e7992fa824b3138d0d86ef7a400b5e12502c92ff708369a345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
