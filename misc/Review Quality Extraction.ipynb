{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a609d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def env_vars():\n",
    "    with open('.env', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    lines = [re.sub('\"|\\n', '', x) for x in lines]\n",
    "    lines = [{x.split('=')[0]: x.split('=')[1]} for x in lines]\n",
    "\n",
    "    env_object = {}\n",
    "    for json_ in lines:\n",
    "        for key, value in json_.items():\n",
    "            env_object[key] = value\n",
    "\n",
    "    return env_object\n",
    "\n",
    "ENV_VARS = env_vars()\n",
    "OPEN_AI_KEY=ENV_VARS['OPEN_AI_KEY']\n",
    "RAPID_API_KEY=ENV_VARS['RAPID_API_KEY']\n",
    "HUGGINGFACEHUB_API_TOKEN=ENV_VARS['HUGGINGFACEHUB_API_TOKEN']\n",
    "\n",
    "OPEN_AI_KEY = os.getenv('OPEN_AI_KEY', default='')\n",
    "os.environ['OPENAI_API_KEY'] = OPEN_AI_KEY\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN', default='')\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37948c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####Overview\n",
    "\n",
    "#~Get Reviews\n",
    "\n",
    "#~Process Reviews\n",
    "\n",
    "#~Lemmatize reviews and remove stopwords\n",
    "\n",
    "#~Find most common words with spacy ----try tfidf vectorizer and use top 5% of the words\n",
    "\n",
    "#~Find phrases around most common words ----graph theory \n",
    "\n",
    "#~Sample phrases so each common keyword is represented adequately\n",
    "\n",
    "#~Use GPT to generate labels\n",
    "\n",
    "#~Assign labels with BERT / hugging face model zero shot learning classification\n",
    "\n",
    "#~Assign sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~Get Reviews\n",
    "#~Process Reviews\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "reviews_df = pd.read_csv('boxing_glove_reviews.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~Lemmatize reviews and remove stopwords\n",
    "import spacy\n",
    "import re\n",
    " \n",
    "spacy_nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def remove_stops(doc):\n",
    "    # Filter out stop words by using the `token.is_stop` attribute\n",
    "    return [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "def lemmatize(doc):\n",
    "    # Take the `token.lemma_` of each non-stop word\n",
    "    return [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "# titles = reviews_df['TITLE'].astype(str).to_list()\n",
    "reviews = reviews_df['REVIEW'].astype(str).to_list()\n",
    "\n",
    "lemmatized_reviews = [re.sub(r'[^\\w\\s]', '', str(x)) for x in reviews]\n",
    "lemmatized_reviews = [lemmatize(spacy_nlp(x)) for x in lemmatized_reviews]\n",
    "lemmatized_reviews = [' '.join(x) for x in lemmatized_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~Find most common words with spacy\n",
    "from collections import Counter\n",
    "\n",
    "# 'NOUN', 'ADJ'\n",
    "def find_types_of_words(text, word_type_list, nlp=spacy_nlp):\n",
    "    doc = nlp(text)\n",
    "    words = [x.text.lower() for x in doc if x.pos_ in word_type_list]\n",
    "    words = list(set(words))\n",
    "    words.sort()\n",
    "    ignore_words = ['have']\n",
    "    words = [x for x in words if x not in ignore_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def most_common_words(text_list, word_type_list):\n",
    "    all_review_adjectives = []\n",
    "    for review in text_list:\n",
    "        adjs = find_types_of_words(review, word_type_list)\n",
    "        all_review_adjectives.extend(adjs)\n",
    "    most_common_words = Counter(all_review_adjectives).most_common(500)\n",
    "    return most_common_words\n",
    "\n",
    "# title_top_nouns_adjs_verbs = most_common_words(titles, ['NOUN', 'ADJ', 'VERB'])\n",
    "review_top_nouns_adjs_verbs = most_common_words(lemmatized_reviews, ['NOUN', 'ADJ', 'VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~Sample reviews so each common keyword is represented adequately\n",
    "import re\n",
    "\n",
    "review_top_nouns_adjs_verbs_top_twenty = review_top_nouns_adjs_verbs[0:20]\n",
    "review_top_nouns_adjs_verbs_vals = [x[0] for x in review_top_nouns_adjs_verbs_top_twenty]\n",
    "review_top_nouns_adjs_verbs_regex = '(?:' + '|'.join(review_top_nouns_adjs_verbs_vals) + ')'\n",
    "phrase_around_top_nouns_adjs_verbs_regex = '[ 0-9a-zA-Z]{1,50} ' + review_top_nouns_adjs_verbs_regex + '[ 0-9a-zA-Z]{1,50}[ \\\\.]{1}'\n",
    "\n",
    "def phrase_around_top_words(review, regex):\n",
    "    try:\n",
    "        return re.findall(regex, review)\n",
    "    except:\n",
    "        return ['']\n",
    "\n",
    "# review_top_nouns_adjs_verbs\n",
    "starttime = datetime.now()\n",
    "_phrases_around_keywords = []\n",
    "for x in lemmatized_reviews[0:1500]:\n",
    "    _phrases_around_keywords.extend(phrase_around_top_words(x, phrase_around_top_nouns_adjs_verbs_regex))\n",
    "\n",
    "phrases_around_keywords = []\n",
    "for x in _phrases_around_keywords[0:1500]:\n",
    "    temp = {\n",
    "        'keyword': re.findall(review_top_nouns_adjs_verbs_regex, x)[0],\n",
    "        'phrase': x\n",
    "    }\n",
    "    phrases_around_keywords.append(temp)\n",
    "\n",
    "phrases_around_keywords_df = pd.DataFrame(phrases_around_keywords)\n",
    "\n",
    "print('Total time to finish:\\t', str(datetime.now() - starttime))\n",
    "# phrases_around_keywords_df\n",
    "\n",
    "_sampled_phrases = phrases_around_keywords_df.groupby('keyword').head(8)['phrase'].to_list()\n",
    "sampled_phrases = []\n",
    "\n",
    "counter = 0\n",
    "for x in _sampled_phrases:\n",
    "    counter += 1\n",
    "    sampled_phrases.append(str(counter) + ') ' + x)\n",
    "\n",
    "sum([len(x) for x in sampled_phrases])\n",
    "# for x in sampled_phrases[0:20]:\n",
    "#     print(x)\n",
    "#     print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~Use GPT to generate labels\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "create_topics_template = \"\"\"\n",
    "    Question: Please create 10 features that describe mostly frequently mentioned qualities of the product from the reviews below. Please output the categories as a numbered list separated by newline characters Reviews: {reviews}\n",
    "    \n",
    "    Answer: Here are 10 features:\n",
    "\"\"\"\n",
    "openai_llm = OpenAI(verbose=True, temperature=.1, model_name=\"text-davinci-003\")\n",
    "simple_prompt = PromptTemplate(input_variables=[\"reviews\"], template=create_topics_template)\n",
    "# openai_llm = ChatOpenAI(verbose=True, temperature=.1, model_name=\"gpt-3.5-turbo\")\n",
    "chain = LLMChain(llm=openai_llm, prompt=simple_prompt)\n",
    "_features = chain.run(' '.join(sampled_phrases))\n",
    "features = _features.split('\\n')\n",
    "features = [re.sub('[0-9]{1,2}\\\\. ', '', x) for x in features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f82164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~Assign labels with BERT zero shot learning classification\n",
    "from transformers import pipeline\n",
    "\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "# classifier = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-deberta-base')\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-1\")\n",
    "\n",
    "print(reviews[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1338bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_start_time = datetime.now()\n",
    "for rev in reviews[0:20]:\n",
    "    incremental_start_time = datetime.now()\n",
    "    x = classifier(rev, candidate_labels=features)['labels'][0]\n",
    "    print('time for increment:\\t', str(datetime.now() - incremental_start_time))\n",
    "    print(x)\n",
    "\n",
    "\n",
    "print('time for total:\\t', str(datetime.now() - total_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245aba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_start_time = datetime.now()\n",
    "# x_revs = reviews[0:10]\n",
    "# x = classifier(x_revs, candidate_labels=features[0:10])#['labels'][0]\n",
    "\n",
    "# print('time for total:\\t', str(datetime.now() - total_start_time))\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65769a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews_df = reviews_df.head(500)\n",
    "sample_reviews_df['TOPIC'] = sample_reviews_df.apply(lambda x: classifier(str(x['REVIEW']), candidate_labels=features)['labels'][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = sample_reviews_df['REVIEW'].to_list()[0]\n",
    "sample_topic = sample_reviews_df['TOPIC'].to_list()[0]\n",
    "\n",
    "def subtopic_labler(review, topic):\n",
    "    review = str(review)\n",
    "    topic = str(topic)\n",
    "    \n",
    "    subtopic_regex = ' [ 0-9a-zA-Z]{1,20} ' + topic.lower() + '[^ \\..]{0,20}'\n",
    "    try:\n",
    "        return re.findall(subtopic_regex, review.lower())[0]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "subtopic_labler(sample_review, sample_topic)\n",
    "\n",
    "sample_reviews_df['SUBTOPIC'] = sample_reviews_df.apply(lambda x: subtopic_labler(x['REVIEW'], x['TOPIC']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = sample_reviews_df['SUBTOPIC'].to_list()\n",
    "# test = [x for x in test if x!='']\n",
    "# for x in test:\n",
    "#     print(x)\n",
    "\n",
    "subtopic_rnd = sample_reviews_df[sample_reviews_df['SUBTOPIC']!=''].groupby('TOPIC').head(3)[['TOPIC', 'SUBTOPIC']]\n",
    "subtopic_rnd.sort_values('TOPIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99638d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set([x[0] for x in review_top_nouns_adjs_verbs]))\n",
    "word_list.sort()\n",
    "word_list = ', '.join(word_list)\n",
    "word_list\n",
    "\n",
    "prompt = \"\"\"\n",
    "    I am going to give you a list of words. Please tell me which refer to people (\"who\"), which refer to dates or times (\"when\"), which refer to places (\"where\"), and which describe actions or activities (\"activities\"). Please return your response as a JSON object with who, when, where, and what as keys, and the results as lists for values. If a word does not fit into one of those categories, you can exclude it from the response. Please exclude adjectives from your response. Words: \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
